[
  {
    "path": "posts/2023-12-04-advanced-regression-modeling-in-r-exploring-beyond-linear-models/",
    "title": "Advanced Regression Modeling in R: Exploring Beyond Linear Models",
    "description": "This post delves into advanced regression techniques in R, offering insights into polynomial regression, logistic regression, and generalized linear models. It aims to provide a comprehensive guide for analysts and data scientists looking to extend their modeling capabilities beyond traditional linear regression.",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-12-04",
    "categories": [
      "Advanced regression"
    ],
    "contents": "\nIntroduction\nLinear regression is a powerful tool, but often, real-world data require more sophisticated models. This post explores advanced regression techniques in R, including polynomial regression, logistic regression, and generalized linear models (GLMs), to tackle diverse data challenges.\nPolynomial Regression\nUnderstanding Polynomial Regression\nPolynomial regression allows us to model relationships that are not linear. It’s particularly useful for capturing curvatures in data.\n\n\nlibrary(ggplot2)\ndata(mtcars)\n\n# Fitting a polynomial regression model\npoly_model <- lm(mpg ~ poly(wt, 2), data=mtcars)\n\n# Plotting\nggplot(mtcars, aes(x=wt, y=mpg)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", formula=y ~ poly(x, 2), se=FALSE) +\n  labs(title=\"Polynomial Regression Example\", x=\"Weight\", y=\"Miles per Gallon\")\n\n\n#Result\npoly_model\n\n\nCall:\nlm(formula = mpg ~ poly(wt, 2), data = mtcars)\n\nCoefficients:\n (Intercept)  poly(wt, 2)1  poly(wt, 2)2  \n      20.091       -29.116         8.636  \n\nInterpreting the Polynomial Regression\n(Intercept) - 20.091: This is the intercept term of the model. It represents the predicted value of mpg (miles per gallon) when the model’s variables are at their mean values. In this case, it indicates the predicted average value of mpg when the weight (wt) is at its mean.\npoly(wt, 2)1 - -29.116: This coefficient represents the linear effect of weight on mpg. The fact that it is negative (-29.116) suggests that an increase in the car’s weight is generally associated with a decrease in fuel efficiency (fewer miles per gallon). The value of this coefficient indicates the magnitude of this effect.\npoly(wt, 2)2 - 8.636: This coefficient represents the quadratic term in the relationship between weight and mpg. A positive coefficient (8.636) for this quadratic term suggests that the relationship between weight and mpg is not simply linear but exhibits curvature. This could mean, for example, that for very light or very heavy weights, the effect of weight on mpg changes differently compared to average weights.\nIn summary, these results indicate that mpg decreases with increasing weight, but not at a constant linear rate. The decrease in mpg is more pronounced for certain ranges of weight than for others. Therefore, this model is better suited to capture the nonlinear relationship between the weight of cars and their fuel efficiency compared to a simple linear model.\nLogistic Regression\nExploring Logistic Regression\nLogistic regression is used for binary classification problems. It predicts the probability of a binary outcome.\nHypothetical Dataset for Logistic Regression Example\nImagine a dataset representing the likelihood of customers subscribing to a new service based on various factors:\nVariables (Columns):\nage: Age of the customer (numeric).\nincome: Annual income of the customer (numeric).\naccount_years: Number of years the customer has held an account with the company (numeric).\nhas_referral: Whether the customer signed up through a referral (binary: 1 for “yes”, 0 for “no”).\nis_subscriber: Whether the customer subscribed to the service (binary: 1 for “yes”, 0 for “no”).\n\n\n\n# generate seed\nset.seed(0)\n\n# Artificial dataset\nage <- sample(18:70, 100, replace = TRUE)\nincome <- sample(30000:100000, 100, replace = TRUE)\naccount_years <- sample(1:30, 100, replace = TRUE)\nhas_referral <- sample(0:1, 100, replace = TRUE)\nis_subscriber <- sample(0:1, 100, replace = TRUE)\n\n# create dataframe\ncustomer_data <- data.frame(age, income, account_years, has_referral, is_subscriber)\n\n# display dataframe\nhead(customer_data)\n\n  age income account_years has_referral is_subscriber\n1  31  58277             8            1             1\n2  21  96393            21            0             1\n3  56  41366            30            1             0\n4  18  43601             5            0             0\n5  51  35050            10            1             0\n6  40  46919            12            1             0\n\n# Fitting a logistic regression model\nlogistic_model <- glm(is_subscriber ~ age + income + account_years + has_referral, \n                      data=customer_data, family=\"binomial\")\n\n# Summary\nsummary(logistic_model)\n\n\nCall:\nglm(formula = is_subscriber ~ age + income + account_years + \n    has_referral, family = \"binomial\", data = customer_data)\n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -1.324e+00  9.787e-01  -1.353    0.176\nage            1.523e-02  1.460e-02   1.044    0.297\nincome         9.608e-06  9.947e-06   0.966    0.334\naccount_years -8.548e-03  2.339e-02  -0.365    0.715\nhas_referral   2.547e-01  4.109e-01   0.620    0.535\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.59  on 99  degrees of freedom\nResidual deviance: 136.06  on 95  degrees of freedom\nAIC: 146.06\n\nNumber of Fisher Scoring iterations: 4\n\nInterpreting the Logistic Regression\nThe output of the logistic regression model fitted to the customer_data dataset can be interpreted as follows:\nCoefficients:\n(Intercept) -1.324e+00: This is the intercept term. It represents the log odds of a customer being a subscriber when all other predictor variables are at their average values. The negative sign indicates that, at the mean values of the predictors, the log odds of being a subscriber are negative, suggesting a lower probability of subscription.\nage 1.523e-02: This coefficient for age indicates that for each additional year in age, the log odds of being a subscriber increase by 0.01523. However, this effect is not statistically significant (p = 0.297), suggesting that age might not be a strong predictor of subscription in this model.\nincome 9.608e-06: The coefficient for income suggests a very small positive effect on the log odds of being a subscriber. For every additional unit increase in income, the log odds increase by approximately 0.000009608. This effect is also not statistically significant (p = 0.334).\naccount_years -8.548e-03: This coefficient indicates that each additional year of having an account decreases the log odds of being a subscriber by 0.008548. Again, this is not statistically significant (p = 0.715).\nhas_referral 2.547e-01: The positive coefficient for has_referral suggests that having a referral is associated with an increase in the log odds of being a subscriber. The magnitude of this increase is 0.2547, but this is not statistically significant (p = 0.535).\n\nModel Fit and Significance:\nThe p-values for all predictors are above the common significance level (0.05), indicating that none of the predictors have a statistically significant effect on the likelihood of being a subscriber in this model.\nThe AIC (Akaike Information Criterion) of the model is 146.06, which can be used for model comparison purposes.\n\nOverall Interpretation:\nThis particular model does not provide strong evidence to suggest that any of the included predictors (age, income, account years, referral status) significantly influence the likelihood of a customer being a subscriber.\nThe lack of significant predictors may indicate that other factors not included in the model might be more influential, or that the relationships are not captured well by a logistic regression model in this case.\nThe model’s predictive power and its usefulness should be further evaluated using other metrics like confusion matrix, ROC curve, etc.\n\nGeneralized Linear Models (GLMs)\nUtilizing GLMs in R\nGLMs extend linear models to support non-normal distributions, making them suitable for a range of data types.\n\n\n# generate the seed \nset.seed(0)\n\n# Artificial dataset\nexposure <- sample(1:10, 100, replace = TRUE)\ngroup <- sample(c(\"A\", \"B\"), 100, replace = TRUE)\n\n# dependent variable\nrate <- rpois(100, lambda = exposure * ifelse(group == \"A\", 0.5, 1))\n\n# create dataframe by combine variable\npoisson_data <- data.frame(group, exposure, rate)\n\n# display dataframe\nhead(poisson_data)\n\n  group exposure rate\n1     A        9    8\n2     B        4    5\n3     A        7    6\n4     A        1    0\n5     A        2    1\n6     A        7    3\n\n# Fitting a GLM\npoisson_glm <- glm(rate ~ exposure + group, family = poisson(), data = poisson_data)\n\n# Summary\nsummary(poisson_glm)\n\n\nCall:\nglm(formula = rate ~ exposure + group, family = poisson(), data = poisson_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.31936    0.17402  -1.835   0.0665 .  \nexposure     0.20923    0.01992  10.505  < 2e-16 ***\ngroupB       0.67281    0.09986   6.737 1.61e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 269.24  on 99  degrees of freedom\nResidual deviance: 124.17  on 97  degrees of freedom\nAIC: 418.79\n\nNumber of Fisher Scoring iterations: 5\n\nInterpreting the GLM\nThe output from fitting a Poisson GLM to the poisson_data dataset can be interpreted as follows:\nCoefficients:\n(Intercept) -0.31936: This intercept term represents the log rate of the outcome (in this case, rate) when all predictor variables are at zero. Since exposure cannot be zero in practical scenarios, the interpretation of the intercept is more theoretical in this context.\nexposure 0.20923: The coefficient for exposure is positive and highly significant (p < 2e-16). This suggests that with each unit increase in exposure, the log rate of rate increases by 0.20923. In practical terms, higher exposure leads to an increase in the rate of the event or count being modeled.\ngroupB 0.67281: The positive and significant coefficient for groupB (p = 1.61e-11) indicates that being in group B, as opposed to group A, is associated with an increase in the log rate of rate by 0.67281. This suggests that the rate of the event or count is higher for group B than for group A.\n\nModel Fit and Significance:\nThe significance codes indicate that both exposure and groupB are highly significant predictors in this model.\nThe null deviance and residual deviance can be used to assess the model fit. A lower residual deviance compared to the null deviance suggests that the model explains a significant portion of the variability in the data.\nThe Akaike Information Criterion (AIC) of 418.79 can be used for comparing this model with other models.\n\nOverall Interpretation:\nThe model suggests that both the level of exposure and the categorical group membership (group A vs. B) significantly influence the rate of the event or count being modeled.\nThe positive coefficients for both exposure and groupB indicate that increases in these predictors are associated with increases in the event rate.\nSince the dispersion parameter is taken to be 1, it is assumed that the mean and variance of the response variable (rate) are equal, as is typical for a Poisson distribution. However, it’s always good practice to check this assumption.\n\nIn summary, the Poisson GLM provides valuable insights into the factors affecting the rate of occurrences in the dataset, with both exposure and group category playing significant roles.\nConclusion\nAdvanced regression techniques in R open up a world of possibilities for data analysis. They allow us to model complex relationships and make more accurate predictions.\n\n\n\n",
    "preview": "posts/2023-12-04-advanced-regression-modeling-in-r-exploring-beyond-linear-models/advanced-regression-modeling-in-r-exploring-beyond-linear-models_files/figure-html5/poly_regression-1.png",
    "last_modified": "2023-12-04T13:26:15+01:00",
    "input_file": "advanced-regression-modeling-in-r-exploring-beyond-linear-models.knit.md"
  },
  {
    "path": "posts/2023-11-27-data-visualization-in-r-enhancing-your-regression-analysis/",
    "title": "Data Visualization in R: Enhancing Your Regression Analysis",
    "description": "Dive into advanced data visualization techniques in R to enhance the interpretation of linear regression analysis results. Using the `mtcars` dataset, we explore how ggplot2 and plotly can provide deeper insights into regression models.",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-11-27",
    "categories": [
      "Linear regression",
      "Visualization"
    ],
    "contents": "\nIntroduction\nData visualization is a powerful tool in statistical analysis, especially in the context of regression analysis. It helps in identifying trends, patterns, and potential issues in the data. In this post, we’ll explore various data visualization techniques using the mtcars dataset in R, demonstrating how these techniques can enhance our understanding and presentation of regression analysis results.\nVisualizing Data Before Regression\nScatter Plots\nBefore diving into regression, it’s crucial to understand the relationship between variables. Scatter plots are a great way to visualize these relationships.\n\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point() +\n  labs(title=\"Scatter plot of MPG vs Weight\", x=\"Weight\", y=\"Miles per Gallon\")\n\n\n\nThis plot shows the relationship between the weight of the cars (wt) and their fuel efficiency (mpg).\nBox Plots and Histograms\nBox plots and histograms provide insights into the distribution of variables, which is important for regression analysis.\n\n\npar(mfrow=c(1,2))\nboxplot(mtcars$mpg, main=\"Boxplot of MPG\")\nhist(mtcars$mpg, main=\"Histogram of MPG\", xlab=\"Miles per Gallon\")\n\n\n\nThe box plot and histogram of mpg indicate the spread and central tendency of fuel efficiency across different cars.\nVisualizing Linear Regression Results\nScatter Plots with Regression Lines\nVisualizing the regression line alongside the data points helps in assessing the model fit.\n\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=TRUE) +\n  labs(title=\"Scatter plot with Regression Line\", x=\"Weight\", y=\"Miles per Gallon\")\n\n\n\nThis plot illustrates how well the linear model fits the data.\nEnhancing Plots with ggplot2\nEnhancing plots with ggplot2 can make interpretations clearer and more intuitive.\n\n\nggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point(color=\"blue\") +\n  geom_smooth(method=\"lm\", se=TRUE, color=\"red\") +\n  labs(title=\"Enhanced Scatter plot with ggplot2\", x=\"Weight\", y=\"Miles per Gallon\")\n\n\n\nThe enhanced plot provides a clearer visualization of the regression line and data points.\nResidual Analysis\nUnderstanding and Visualizing Residuals\nAnalyzing residuals is key to understanding the performance of a regression model.\n\n\nmodel <- lm(mpg ~ wt, data=mtcars)\nresiduals_plot <- ggplot(model, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept=0, linetype=\"dashed\") +\n  labs(title=\"Residual Plot\", x=\"Fitted Values\", y=\"Residuals\")\nresiduals_plot\n\n\n\nThe residual plot helps to diagnose issues with the model, like non-linearity or heteroscedasticity.\nChecking for Normality of Residuals\nIt’s important to check if the residuals from our regression model are normally distributed. This can be done using a QQ plot.\n\n\nlibrary(ggplot2)\n\nmodel <- lm(mpg ~ wt, data=mtcars)\nresiduals_data <- as.data.frame(qqnorm(resid(model), plot.it = FALSE))\nnames(residuals_data) <- c(\"Theoretical\", \"Residuals\")\n\n# Calculate the standard deviation of residuals\nresiduals_std <- sd(residuals_data$Residuals)\n\n# Flag outliers - points more than 2 standard deviations from the mean\nresiduals_data$Outlier <- abs(residuals_data$Residuals) > 2 * residuals_std\n\n# Creating the enhanced QQ plot\nggplot(residuals_data, aes(x = Theoretical, y = Residuals)) +\n  geom_point(aes(color = Outlier)) +\n  geom_qq_line(aes(sample = Residuals)) +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  theme_minimal() +\n  labs(title=\"Enhanced QQ Plot for Residuals\", x=\"Theoretical Quantiles\", y=\"Sample Quantiles\")\n\n\n\nInteractive Visualizations with Plotly\nCreating Interactive Plots\nInteractive plots make it easier to explore complex data and models.\n\n\nlibrary(plotly)\np <- ggplot(mtcars, aes(x=wt, y=mpg)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=TRUE)\nggplotly(p)\n\n\n\nWith Plotly, users can interact with the plot to gain deeper insights.\nConclusion\nEffective data visualization is crucial in regression analysis. It aids in understanding the data, diagnosing the model, and communicating results. R, with its robust packages like ggplot2 and Plotly, offers a wide array of tools for creating meaningful and insightful visualizations.\n\n\n\n",
    "preview": "posts/2023-11-27-data-visualization-in-r-enhancing-your-regression-analysis/data-visualization-in-r-enhancing-your-regression-analysis_files/figure-html5/scatter_plot-1.png",
    "last_modified": "2023-12-04T13:36:22+01:00",
    "input_file": "data-visualization-in-r-enhancing-your-regression-analysis.knit.md"
  },
  {
    "path": "posts/2023-10-27-linear-regression-analysis-with-r/",
    "title": "Linear Regression Analysis with R: Predicting Sales",
    "description": "Linear regression analysis is a fundamental statistical technique used to model the relationship between a dependent variable and one or more independent variables. In this post, we will explore how to perform linear regression analysis using R to predict sales based on various factors.",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-10-27",
    "categories": [
      "Linear regression",
      "ggplot2"
    ],
    "contents": "\nCreate sample sales data\n\n\nset.seed(123)\nsales_data <- data.frame(\n  Sales = rnorm(100, mean = 100, sd = 20),\n  Advertising = rnorm(100, mean = 50, sd = 10),\n  Seasonality = rep(c(\"Summer\", \"Fall\", \"Winter\", \"Spring\"), each = 25)\n)\n\n\nExploratory Data Analysis\nBefore building our model, let’s explore the data:\nCheck the summary statistics\n\n\nsummary(sales_data)\n\n     Sales         Advertising    Seasonality       \n Min.   : 53.82   Min.   :29.47   Length:100        \n 1st Qu.: 90.12   1st Qu.:41.99   Class :character  \n Median :101.24   Median :47.74   Mode  :character  \n Mean   :101.81   Mean   :48.92                     \n 3rd Qu.:113.84   3rd Qu.:54.68                     \n Max.   :143.75   Max.   :82.41                     \n\nNote: The summary statistics for the variables Sales, Advertising, and Seasonality reveal the following important information succinctly:\nSales:\nSales vary significantly, ranging from approximately 53.82 to 143.75.\nThe distribution of sales appears relatively symmetric, with a mean close to the median.\nSales show moderate dispersion around the mean.\nAdvertising:\nAdvertising expenses vary significantly, ranging from approximately 29.47 to 82.41.\nThe distribution of advertising expenses is not mentioned as either symmetric or non-symmetric in the summary.\nAdvertising expenses exhibit moderate dispersion around the mean.\nSeasonality:\nThe Seasonality variable is categorical, representing seasons (e.g., “Summer,” “Fall”).\nThe summary indicates that the variable is of type “character,” and its mode is “character.”\nExamine the correlation between variables\n\n\ncor(sales_data$Sales, sales_data$Advertising)\n\n[1] -0.04953215\n\nNote: In summary, the correlation coefficient of approximately -0.0495 suggests a weak negative relationship between Sales and Advertising expenses, but the correlation is so close to zero that it is unlikely to have practical significance in explaining Sales variability based on Advertising expenses alone.\nVisualize the data distribution\n\n\nggplot(sales_data, aes(x = Sales)) + geom_histogram(binwidth = 10, fill = \"blue\", \n                                                    color = \"black\")\n\n\n\nNote: In summary, The data is somewhat normally distributed but leans slightly to the left (negatively skewed). the sales distribution suggests that the most common sales values are between 75 and 125, with fewer occurrences of extremely low or high sales\nBuilding the Linear Regression Model\nNow, let’s construct the linear regression model:\n\n\n# Fit the linear regression model\nmodel <- lm(Sales ~ Advertising + Seasonality, data = sales_data)\nmodel\n\n\nCall:\nlm(formula = Sales ~ Advertising + Seasonality, data = sales_data)\n\nCoefficients:\n      (Intercept)        Advertising  SeasonalitySpring  \n        106.36304           -0.09059            3.69334  \nSeasonalitySummer  SeasonalityWinter  \n         -2.75086           -1.43368  \n\nNote: Linear regression model in R aims to explain the variable Sales as a function of Advertising and Seasonality.\n(Intercept): This is the expected value of Sales when all other predictors are set to zero. In this case, if there’s no advertising and considering the reference category for seasonality (which is not shown, but we can infer might be ‘Autumn’ or ‘Fall’ given the other categories listed), the expected sales value is approximately 106.36304.\nAdvertising: The coefficient for Advertising is -0.09059. This means that for every one-unit increase in advertising, we can expect a decrease of 0.09059 in sales, assuming other factors remain constant. This is a bit counter-intuitive because we’d generally expect advertising to increase sales. However, this could be due to various reasons like ineffective advertising, the presence of other confounding variables, or multicollinearity.\nSeasonalitySpring: The coefficient is 3.69334. This indicates that, relative to the reference season (presumably ‘Autumn’), sales in the Spring season are expected to be higher by approximately 3.69334 units, given that other factors remain constant.\nSeasonalitySummer: The coefficient is -2.75086. Compared to the reference season, sales in the Summer are expected to decrease by approximately 2.75086 units, assuming all else remains the same.\nSeasonalityWinter: The coefficient is -1.43368. This means that, relative to the reference season, sales in Winter are expected to be lower by about 1.43368 units, keeping everything else constant.\nIn summary:\nThe model suggests that advertising has a negative impact on sales, which may warrant further investigation.\nSpring seems to be a favorable season for sales compared to Autumn, while both Summer and Winter appear to be less favorable.\nModel Evaluation\nEvaluate the model by examining:\nResidual plots\n\n\nplot(model, which = 1)\n\n\n\nNote: the residuals show a generally good fit. The residuals are scattered around the zero line without a clear systematic pattern, suggesting that the linear model is appropriate for the data. Any slight deviation might be due to random noise, which is expected.\nAssumptions of linear regression\n\n\nplot(model, which = 2)\n\n\n\nNote: The residuals are approximately normally distributed, which supports the validity of the linear regression model.\nModel performance metrics\n\n\nsummary(model)\n\n\nCall:\nlm(formula = Sales ~ Advertising + Seasonality, data = sales_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46.52 -11.20  -0.47  12.79  40.94 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       106.36304   10.03715  10.597   <2e-16 ***\nAdvertising        -0.09059    0.19573  -0.463    0.645    \nSeasonalitySpring   3.69334    5.22139   0.707    0.481    \nSeasonalitySummer  -2.75086    5.21896  -0.527    0.599    \nSeasonalityWinter  -1.43368    5.29078  -0.271    0.787    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.45 on 95 degrees of freedom\nMultiple R-squared:  0.02003,   Adjusted R-squared:  -0.02123 \nF-statistic: 0.4855 on 4 and 95 DF,  p-value: 0.7464\n\nNote: This output is the summary of a linear regression model with the dependent variable Sales and independent variables Advertising and Seasonality. Let’s interpret the key parts:\nResiduals:\nThe residuals range from a minimum of -46.52 to a maximum of 40.94.\nThe median residual is very close to zero (-0.47), which is a good sign, indicating that the model doesn’t systematically overpredict or underpredict sales.\n\nCoefficients:\n(Intercept): The estimated intercept is 106.36304. This represents the expected sales when all other predictors are zero. Given the nature of the predictors, this value might not have a meaningful real-world interpretation.\nAdvertising: For each unit increase in advertising, sales are expected to decrease by 0.09059 units. However, the p-value is 0.645, suggesting that this effect is not statistically significant.\nSeasonality:\nSpring: Sales are expected to be higher by 3.69334 units in spring compared to the reference season (probably ‘Autumn’ since it’s not listed). This is not statistically significant (p-value = 0.481).\nSummer: Sales are expected to be lower by 2.75086 units in summer compared to the reference season. This is also not statistically significant (p-value = 0.599).\nWinter: Sales are expected to be lower by 1.43368 units in winter compared to the reference season. Again, this is not statistically significant (p-value = 0.787).\n\n\nModel Significance:\nThe Multiple R-squared value is 0.02003, which indicates that only about 2% of the variance in Sales is explained by the model. This is quite low.\nThe Adjusted R-squared is negative, suggesting that the model doesn’t provide a better fit to the data than a simple mean model.\nThe overall F-statistic tests the hypothesis that all regression coefficients are equal to zero (i.e., no effect). With a p-value of 0.7464, we fail to reject this hypothesis, suggesting that the predictors, as a whole, do not provide significant information in predicting Sales.\n\nIn summary:\nThe predictors Advertising and Seasonality do not seem to be significant predictors of Sales in this model. The model, as it stands, might not be a good fit for the data.\nFurther exploration might be needed, perhaps considering other variables or looking into potential non-linear relationships or interactions.\nWe’ll come back to this model in another post to see if we can’t improve it. Let’s move on and assume this model is adequate to fit data.\nPredicting Sales\nUse the model to predict sales based on new data:\n\n\n# Predict sales for new data\nnew_data <- data.frame(\n  Advertising = c(60, 70, 80),\n  Seasonality = c(\"Summer\", \"Fall\", \"Winter\")\n)\npredictions <- predict(model, newdata = new_data)\npredictions\n\n        1         2         3 \n 98.17672 100.02167  97.68208 \n\nNote: The prediction results based on the linear regression model are as follows:\nFor the first observation (1), with an advertising expenditure of 60 and the season being “Summer,” the predicted sales are approximately 98.18.\nFor the second observation (2), with an advertising expenditure of 70 and the season being “Fall,” the predicted sales are approximately 100.02.\nFor the third observation (3), with an advertising expenditure of 80 and the season being “Winter,” the predicted sales are approximately 97.68.\nThese predictions are based on the linear regression model that you previously fitted using the training data. The model attempts to make predictions based on the provided values of advertising expenditure and seasonality for each observation. It’s important to note that these predictions are based on the linear relationships modeled by the model, but the quality of predictions may vary depending on the accuracy of the model and the validity of the input data.\nConclusion\nLinear regression analysis is a valuable technique for understanding and predicting relationships between variables. In this post, we’ve covered the entire process of performing linear regression analysis with R, from creating sample data to making predictions.\nRemember that effective interpretation of results and continuous refinement of models are essential for successful data analysis.\n\n\n\n",
    "preview": "posts/2023-10-27-linear-regression-analysis-with-r/linear-regression-analysis-with-r_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-11-27T09:17:04+01:00",
    "input_file": "linear-regression-analysis-with-r.knit.md"
  },
  {
    "path": "posts/2023-10-20-data-visualization-with-r/",
    "title": "Data Visualization with R",
    "description": "Data visualization is a powerful tool in the data analyst's toolkit. It allows us to quickly understand trends, patterns, and potential anomalies in our data. A well-crafted chart can reveal hidden insights and tell a story that numbers alone cannot.",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-10-13",
    "categories": [
      "visualization",
      "ggplot2"
    ],
    "contents": "\nUsing ggplot2\nggplot2 is a popular data visualization package in R that provides a flexible and consistent way to create a wide variety of charts.\n\n\n# Loading necessary libraries\nlibrary(ggplot2)\n\n\nInstallation and Loading\nIf you haven’t already installed ggplot2, you can do so with the following command:\n\n\n# Loading necessary libraries\ninstall.packages(\"ggplot2\")\n\n\nOnce installed, you can load the library:\n\n\nlibrary(ggplot2)\n\n\nCreating Basic Charts\nHistograms\nFunction: geom_histogram()\nDescription: This function allows us to visualize the distribution of a continuous variable.\nSyntax:\n\n\nggplot(data, aes(x=value)) + \n  geom_histogram(binwidth=width, fill=color, color=edge_color, alpha=transparency)\n\n\ndata: The dataset containing the variable to be plotted.\naes(x=value): Specifies the variable to be plotted on the x-axis.\nbinwidth: (Optional) Width of the histogram bins.\nfill: (Optional) Fill color of the bars.\ncolor: (Optional) Border color of the bars.\nalpha: (Optional) Transparency of the bars.\nExample:\n\n\n# Sample data\ndata <- data.frame(value = rnorm(1000))\n\n# Creating a histogram\nggplot(data, aes(x=value)) + \n  geom_histogram(binwidth=0.5, fill=\"blue\", color=\"black\", alpha=0.7) +\n  labs(title=\"Histogram of Values\", x=\"Value\", y=\"Frequency\")\n\n\n\nBar Charts\nFunction: geom_bar()\nDescription: Bar charts are useful for comparing categories.\nSyntax:\n\n\nggplot(data, aes(x=category, y=count)) + \n  geom_bar(stat=\"identity\", fill=color, color=edge_color)\n\n\ndata: The dataset containing the categories and their counts.\naes(x=category, y=count): Specifies the variables for x and y axes.\nstat: Indicates that the heights of the bars represent the counts.\nfill: (Optional) Fill color of the bars.\ncolor: (Optional) Border color of the bars.\nExample:\n\n\n# Sample data\ndata <- data.frame(category = c(\"A\", \"B\", \"C\", \"D\"), count = c(23, 45, 12, 67))\n\n# Creating a bar chart\nggplot(data, aes(x=category, y=count)) + \n  geom_bar(stat=\"identity\", fill=\"skyblue\", color=\"black\") +\n  labs(title=\"Bar Chart of Categories\", x=\"Category\", y=\"Count\")\n\n\n\nScatter Plots\nFunction: geom_point()\nDescription: Scatter plots help visualize the relationship between two continuous variables.\nSyntax:\n\n\nggplot(data, aes(x=x_value, y=y_value)) + \n  geom_point(color=color)\n\n\ndata: The dataset containing the x and y variables.\naes(x=x_value, y=y_value): Specifies the variables for x and y axes.\ncolor: (Optional) Color of the points.\nExample:\n\n\n# Sample data\ndata <- data.frame(x = rnorm(100), y = rnorm(100))\n\n# Creating a scatter plot\nggplot(data, aes(x=x, y=y)) + \n  geom_point(color=\"red\") +\n  labs(title=\"Scatter Plot of x vs y\", x=\"X Values\", y=\"Y Values\")\n\n\n\nConclusion\nIn this post, we’ve just scratched the surface of what’s possible with ggplot2 in R. The package offers a wide range of customization options and chart types to explore. As always, the key to a good visualization is not just how it looks, but the insights it provides. So, keep experimenting and find the best way to tell your data’s story!\n\n\n\n",
    "preview": "posts/2023-10-20-data-visualization-with-r/data-visualization-with-r_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2023-10-20T13:10:21+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-09-29-transforming-and-manipulating-data-in-r/",
    "title": "Transforming and Manipulating Data in R",
    "description": "After importing and describing your data, the next step in your data analysis journey is often to transform and manipulate your data to prepare it for analysis. In this post, we will explore how to handle missing values, clean data, and transform variables.",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-09-29",
    "categories": [
      "transformation",
      "manipulation",
      "cleaning"
    ],
    "contents": "\nGenerating Sample Data\nLet’s start by generating some sample data with missing values, duplicates, and variables to transform:\n\n\n# Setting the seed for reproducibility\nset.seed(123)\n\n# Creating a sample data frame\nsample_data <- data.frame(\n  ID = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3),\n  Age = c(21, 25, 30, 35, 40, 45, 50, 55, 60, 65, 30),\n  Income = c(30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000, 40000),\n  Gender = c(\"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\")\n)\n\n# Introducing some missing values\nsample_data$Age[c(3, 6, 9)] <- NA\nsample_data$Income[c(2, 5, 10)] <- NA\n\n# Viewing the sample data\nsample_data\n\n\nHandling Missing Values\nFunction: is.na()\nSyntax:\n\n\nis.na(data)\n\n\ndata: The data object to be checked for missing values.\nExample: To identify missing values in the sample data:\n\n\nis.na(sample_data)\n\n\nFunction: na.omit()\nSyntax:\n\n\nna.omit(data)\n\n\ndata: The data object from which rows with missing values should be removed.\nExample: To remove rows with missing values from the sample data:\n\n\nna.omit(sample_data)\n\n\nFunction: complete.cases()\nSyntax:\n\n\ncomplete.cases(data)\n\n\ndata: The data object to be checked for complete cases.\nExample: To identify rows with no missing values in the sample data:\n\n\ncomplete.cases(sample_data)\n\n\nData cleaning\nFunction: distinct()\nSyntax:\n\n\ndistinct(data)\n\n\ndata: The data object from which duplicate rows should be removed.\nExample: To remove duplicate rows in the sample data:\n\n\ndplyr::distinct(sample_data)\n\n\nFunction: filter()\nSyntax:\n\n\nfilter(data, condition)\n\n\ndata: The data object to be filtered.\ncondition: The condition to be met for a row to be included in the output.\nExample: To filter rows in the sample data:\n\n\ndplyr::filter(sample_data, Age > 30)\n\n\nFunction: mutate()\nSyntax:\n\n\nmutate(data, new_variable = expression)\n\n\ndata: The data object to be transformed.\nnew_variable: The name of the new or modified variable.\nexpression: The expression defining the new or modified variable.\nExample: To create new variable in the sample data:\n\n\ndplyr::mutate(sample_data, Income_Thousand = Income / 1000)\n\n\nTransforming Variables\nFunction: transmute()\nSyntax:\n\n\ntransmute(data, new_variable = expression)\n\n\ndata: The data object to be transformed.\nnew_variable: The name of the new variable.\nexpression: The expression defining the new variable.\nExample: To create new variable in the sample data:\n\n\ndplyr::transmute(sample_data, Income_Thousand = Income / 1000)\n\n\nFunction: summarise\nSyntax:\n\n\nsummarise(data, summary_statistic = expression)\n\n\ndata: The data object to be summarized.\nsummary_statistic: The name of the summary statistic.\nexpression: The expression defining the summary statistic.\nExample: To create new variable in the sample data:\n\n\ndplyr::summarise(sample_data, Avg_Income = mean(Income, na.rm = TRUE))\n\n\nConclusion\nBy mastering these data transformation and manipulation techniques, you can effectively prepare your data for subsequent analysis and modeling.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-09-29T12:55:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-09-22-describing-data-in-r/",
    "title": "Describing Data in R",
    "description": "Once you have imported your data into R, the next step is often to explore and describe your data. Descriptive statistics and visualizations can help you understand the distribution, central tendency, and spread of your data. Here's how to get started:",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-09-22",
    "categories": [
      "descriptive"
    ],
    "contents": "\nSummary Statistics\nSyntax:\n\n\nsummary(data)\n\n\ndata: The data frame you wish to summarize.\nExample: To summarize the sample data:\n\n\nmy_data <- read.csv(\"./static/sample_data.csv\", header = TRUE, sep = \",\")\nsummary(my_data)\n\n\nStructure of the Data\nSyntax:\n\n\nstr(data)\n\n\ndata: The data frame whose structure you wish to view.\nExample: To view the structure of the sample data:\n\n\nmy_data <- read.csv(\"./static/sample_data.csv\", header = TRUE, sep = \",\")\nstr(my_data)\n\n\nData visualization\nVisualizing your data can provide insights that are not apparent through descriptive statistics alone.\nHistogram\nSyntax:\n\n\nhist(data$variable)\n\n\ndata$variable: The variable you wish to plot a histogram for.\nExample: To plot a histogram of the Age variable in the sample data:\n\n\nmy_data <- read.csv(\"./static/sample_data.csv\", header = TRUE, sep = \",\")\nhist(my_data$Age)\n\n\nBoxplot\nSyntax:\n\n\nboxplot(data$variable)\n\n\ndata$variable: The variable you wish to plot a boxplot for.\nExample: To plot a boxplot of the Age variable in the sample data:\n\n\nmy_data <- read.csv(\"./static/sample_data.csv\", header = TRUE, sep = \",\")\nboxplot(my_data$Age)\n\n\nConclusion\nBy utilizing these functions and visualizations, you can gain a deeper understanding of your data, which is crucial for any subsequent analysis or modeling.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-09-22T15:04:06+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-09-15-importing-data-into-r/",
    "title": "Importing Data into R",
    "description": "One of the first steps in any data analysis project is importing your data into R. Whether you're dealing with CSV files, Excel spreadsheets, or SQL databases, R has got you covered. Here's how to get started:",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-09-15",
    "categories": [
      "import"
    ],
    "contents": "\nImporting CSV Files\nSyntax:\n\n\nread.csv(\"file_path\", header = TRUE, sep = \",\")\n\n\nfile_path: Path to the CSV file you wish to import.\nheader: Whether the first row contains column names. Default is TRUE.\nsep: The field separator character. Default is a comma.\nExample: To import a CSV file:\n\n\nmy_data <- read.csv(\"./static/sample_data.csv\", header = TRUE, sep = \",\")\n\n\nImporting Excel Files\nSyntax:\n\n\nlibrary(readxl)\nread_excel(\"file_path\", sheet = 1)\n\n\nfile_path: Path to the Excel file you wish to import.\nsheet: The sheet number or name to read.\nExample: To import an Excel file:\n\n\nlibrary(readxl)\nmy_excel_data <- read_excel(\"./static/sample_data.xlsx\", sheet = 1)\n\n\nImporting SQL Databases\nSyntax:\n\n\nlibrary(DBI)\ncon <- dbConnect(RSQLite::SQLite(), \"db_path\")\ndbReadTable(con, \"table_name\")\n\n\ndb_path: Path to the SQL database you wish to connect to.\ntable_name: The name of the table you wish to read.\nExample: To connect to a SQL database and read a table:\n\n\nlibrary(DBI)\ncon <- dbConnect(RSQLite::SQLite(), \"./static/sample_data.db\")\nmy_db_data <- dbReadTable(con, \"sample_table\")\n\n\nConclusion\nBy mastering these methods, you can easily import various types of data into R, setting the stage for your data analysis journey.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-09-15T16:49:27+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-08-10-managing-data-in-r/",
    "title": "Managing Packages in R",
    "description": "One of the great advantages of R is its extensive library of packages. These packages extend R's functionalities, offering tools for a variety of analytical tasks. Here's how to manage these packages:",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-08-10",
    "categories": [
      "packages"
    ],
    "contents": "\nInstalling packages\nSyntax:\n\n\ninstall.packages(\"package_name\")\n\n\npackage_name: This parameter represents the name of the package you wish to install.\nExample 1: To install the ggplot2 package\n\n\ninstall.packages(\"ggplot2\")\n\n\nExample 2: To install the dplyr package\n\n\ninstall.packages(\"dplyr\")\n\n\nLoading packages\nSyntax:\n\n\nlibrary(package_name)\n\n\npackage_name: This parameter represents the name of the package you wish to load into your R session.\nExample 1: To load the ggplot2 package\n\n\nlibrary(ggplot2)\n\n\nExample 2: To load the dplyr package\n\n\nlibrary(dplyr)\n\n\nUpdating packages\nSyntax:\n\n\nupdate.packages(ask = FALSE)\n\n\nask: When set to FALSE, R will update all packages without asking for confirmation. If set to TRUE, R will ask for confirmation for each package.\nExample: To update all packages without confirmation\n\n\nupdate.packages(ask = FALSE)\n\n\nRemoving packages\nSyntax:\n\n\nremove.packages(\"package_name\")\n\n\npackage_name: This parameter represents the name of the package you wish to remove.\nExample 1: To remove the ggplot2 package\n\n\nremove.packages(\"ggplot2\")\n\n\nExample 2: To remove the dplyr package\n\n\nremove.packages(\"dplyr\")\n\n\nConclusion\nBy mastering these functions, you can easily manage and maintain your packages in R, ensuring an optimal working environment.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-10T14:37:45+02:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to R-vealing Data",
    "description": "Welcome to my new blog, R-vealing Data. Whether you're a beginner or an expert in R, there are always tips and tricks that can make your work easier.",
    "author": [
      {
        "name": "Cédric Hassen-Khodja",
        "url": {}
      }
    ],
    "date": "2023-08-10",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2023-08-10T14:51:02+02:00",
    "input_file": {}
  }
]
